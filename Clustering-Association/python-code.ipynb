{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 484 :: Data Mining :: George Mason University :: Spring 2023\n",
    "\n",
    "\n",
    "# Homework 3: Clustering&Association Rule Mining\n",
    "\n",
    "- **100 points [9% of your final grade]**\n",
    "- **Due Sunday, April 16 by 11:59pm**\n",
    "\n",
    "- *Goals of this homework:* (1) implement your K-means model; and (2) implement the association rule mining process with the Apriori algorithm.\n",
    "\n",
    "- *Submission instructions:* for this homework, you only need to submit to Blackboard. Please name your submission **FirstName_Lastname_hw3.ipynb**, so for example, my submission would be something like **Ziwei_Zhu_hw3.ipynb**. Your notebook should be fully executed so that we can see all outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Clustering (50 points)\n",
    "\n",
    "In this part, you will implement your own K-means algorithm to conduct clustering on handwritten digit images. In this homework, we will still use the handwritten digit image dataset we have already used in previous homework. However, since clustering is unsupervised learning, which means we do not use the label information anymore. So, here, we will only use the testing data stored in the \"test.txt\" file.\n",
    "\n",
    "First, let's load the data by excuting the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of testing feature matrix: shape (10000, 784)\n",
      "array of testing label matrix: shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test = np.loadtxt(\"test.txt\", delimiter=',')\n",
    "test_features = test[:, 1:]\n",
    "test_labels = test[:, 0]\n",
    "print('array of testing feature matrix: shape ' + str(np.shape(test_features)))\n",
    "print('array of testing label matrix: shape ' + str(np.shape(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time for you to implement your own K-means algorithm. First, please write your code to build your K-means model using the image data with **K = 10**, and **Euclidean distance**.\n",
    "\n",
    "**Note: You should implement the algorithm by yourself. You are NOT allowed to use Machine Learning libraries like Sklearn**\n",
    "\n",
    "**Note: you need to decide when to stop the model training process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First randomly select k data points for initializing centroids.\n",
    "def Centroids(d, k):\n",
    "    \n",
    "    # Amount of data points\n",
    "    num = d.shape[0]\n",
    "    \n",
    "    # Choosing k indexes randomly\n",
    "    indices = np.random.choice(num, k, replace=False)\n",
    "    \n",
    "    # Getting centroids\n",
    "    centroids = d[indices]\n",
    "    \n",
    "    return (centroids)\n",
    "\n",
    "# Euclidean distance method\n",
    "def Euclidean(a, b):\n",
    "    \n",
    "    # Getting the squared distances\n",
    "    difference = ((a - b) ** 2)\n",
    "    \n",
    "    # Summing the squared differences\n",
    "    sum = np.sum(difference, axis=1)\n",
    "    \n",
    "    # Square root for euclidean distance\n",
    "    distance = np.sqrt(sum)\n",
    "    \n",
    "    return (distance)\n",
    "\n",
    "\n",
    "# Method for going through each centroid and distance\n",
    "def Assign(d, c):\n",
    "    \n",
    "    # Storing distances\n",
    "    distances = np.zeros((d.shape[0], c.shape[0]))\n",
    "    \n",
    "    # Getting distances\n",
    "    for i in range(len(c)):\n",
    "        \n",
    "        distances[:, i] = Euclidean(d, c[i])\n",
    "    \n",
    "    # Assigning\n",
    "    assignments = np.argmin(distances, axis=1)\n",
    "    \n",
    "    return (assignments)\n",
    "\n",
    "# Method to update centroid\n",
    "def Update(d, a, k):\n",
    "    \n",
    "    # Storing centroids\n",
    "    new = np.zeros((k, d.shape[1]))\n",
    "    \n",
    "    # Getting mean\n",
    "    for i in range(k):\n",
    "        \n",
    "        ap = d[a == i]\n",
    "        \n",
    "        new[i] = np.mean(ap)\n",
    "    \n",
    "    return (new)\n",
    "\n",
    "# K-means using helpers\n",
    "def kmeans(d, k, max=100, min=0.01):\n",
    "    \n",
    "    # Initializing centroids\n",
    "    centroids = Centroids(d, k)\n",
    "    \n",
    "    # For comparing\n",
    "    previous = np.zeros(d.shape[0])\n",
    "    \n",
    "    # Updating centroids and assignments\n",
    "    for _ in range(max):\n",
    "        # Assigning to clusters\n",
    "        assignments = Assign(d, centroids)\n",
    "        \n",
    "        # Stop when there is convergence\n",
    "        changes = np.sum(assignments != previous)\n",
    "        \n",
    "        if ((changes / d.shape[0]) < min):\n",
    "            \n",
    "            break\n",
    "        \n",
    "        # Updating centroids from current assignments\n",
    "        centroids = Update(d, assignments, k)\n",
    "        \n",
    "        # Updating previous assignments for the next comparison\n",
    "        previous = assignments.copy()\n",
    "    \n",
    "    return (centroids, assignments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to calculate the square root of Sum of Squared Error (SSE) of each cluster generated by your K-means algorithm. Then, print out the averaged SSE of your algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, please have a look on https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_completeness_v_measure.html#sklearn.metrics.homogeneity_completeness_v_measure, and use this function to print out the homogeneity, completeness, and v-measure of your K-means model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Association Rule Mining (50 points)\n",
    "\n",
    "In this part, you are going to examine movies using our understanding of association rules. For this part, you need to implement the apriori algorithm, and apply it to a movie rating dataset to find association rules of user-rate-movie behaviors. First, run the next cell to load the dataset we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of user-movie matrix: shape (11743, 2)\n"
     ]
    }
   ],
   "source": [
    "user_movie_data = np.loadtxt(\"movie_rated.txt\", delimiter=',')\n",
    "print('array of user-movie matrix: shape ' + str(np.shape(user_movie_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, there are two columns: the first column is the integer ids of users, and the second column is the integer ids of movies. Each row denotes that the user of given user id rated the movie of the given movie id. We are going to treat each user as a transaction, so you will need to collect all the movies that have been rated by a single user as a transaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to implement the apriori algorithm and apply it to this dataset to find association rules of user rating behaviors with **minimum support of 0.2** and **minimum confidence of 0.8**. We know there are many existing implementations of apriori online (check github for some good starting points). You are welcome to read existing codebases and let that inform your approach. \n",
    "\n",
    "**Note: Do not copy-paste any existing code.**\n",
    "\n",
    "**Note: We want your code to have sufficient comments to explain your steps, to show us that you really know what you are doing.**\n",
    "\n",
    "**Note: You should add print statements to print out the intermediate steps of your method -- e.g., the size of the candidate set at each step of the method, the size of the filtered set, and any other important information you think will highlight the method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the candidate set (k=1): 408\n",
      "Size of the filtered set (k=1): 21\n",
      "Size of the candidate set (k=2): 210\n",
      "Size of the filtered set (k=2): 36\n",
      "Size of the candidate set (k=3): 105\n",
      "Size of the filtered set (k=3): 12\n",
      "Size of the candidate set (k=4): 7\n",
      "Size of the filtered set (k=4): 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[480.0], [1221.0], [1270.0], [1265.0], [1197.0]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method for getting the initial transaction candidates\n",
    "def Initialize(transactions):\n",
    "    \n",
    "    candidates = []    \n",
    "    \n",
    "    for transaction in transactions:    \n",
    "    \n",
    "        for item in transaction:    \n",
    "    \n",
    "            if [item] not in candidates:    \n",
    "    \n",
    "                candidates.append([item])\n",
    "    \n",
    "    return (candidates)\n",
    "\n",
    "# Method for getting the supports \n",
    "def Support(count, num):\n",
    "    \n",
    "    supports = {}\n",
    "    \n",
    "    for key, value in count.items():\n",
    "    \n",
    "        supports[key] = (value / num)\n",
    "    \n",
    "    return (supports)\n",
    "\n",
    "# Method for filtering the candidates \n",
    "def Filter(transactions, candidates, min_support):\n",
    "    \n",
    "    count = {}\n",
    "    \n",
    "    for transaction in transactions:\n",
    "    \n",
    "        for candidate in candidates:\n",
    "    \n",
    "            if set(candidate).issubset(transaction):\n",
    "    \n",
    "                tuple_candidate = tuple(candidate)\n",
    "    \n",
    "                count[tuple_candidate] = (count.get(tuple_candidate, 0) + 1)\n",
    "                \n",
    "    supports = Support(count, float(len(transactions)))\n",
    "    \n",
    "    filtered = []\n",
    "    \n",
    "    for key, support in supports.items():\n",
    "    \n",
    "        if (support >= min_support):\n",
    "            \n",
    "            filtered.append(list(key))\n",
    "    \n",
    "    return (filtered)\n",
    "\n",
    "# Method for making new candidate sets\n",
    "def New(filtered, k):\n",
    "    \n",
    "    new_candidates = []\n",
    "    \n",
    "    for i in range(len(filtered)):\n",
    "    \n",
    "        for j in range(i+1, len(filtered)):\n",
    "    \n",
    "            L1 = sorted(filtered[i])[:k-2]\n",
    "    \n",
    "            L2 = sorted(filtered[j])[:k-2]\n",
    "    \n",
    "            if (L1 == L2):\n",
    "                \n",
    "                new_candidates.append(sorted(set(filtered[i]) | set(filtered[j])))\n",
    "    \n",
    "    return (new_candidates)\n",
    "\n",
    "# The apriori algorithm using helpers\n",
    "def Apriori(transactions, min=0.2):\n",
    "    \n",
    "    candidates = Initialize(transactions)\n",
    "    \n",
    "    all_frequent_itemsets = []\n",
    "    \n",
    "    k = 2\n",
    "    \n",
    "    while candidates:\n",
    "    \n",
    "        filtered = Filter(transactions, candidates, min)\n",
    "    \n",
    "        # For all candidates\n",
    "        print(f\"Size of the candidate set (k={k-1}): {len(candidates)}\")\n",
    "    \n",
    "        # For all filtered\n",
    "        print(f\"Size of the filtered set (k={k-1}): {len(filtered)}\")\n",
    "    \n",
    "        if not filtered:\n",
    "    \n",
    "            break\n",
    "    \n",
    "        all_frequent_itemsets.extend(filtered)\n",
    "    \n",
    "        candidates = New(filtered, k)\n",
    "    \n",
    "        k += 1\n",
    "    \n",
    "    return (all_frequent_itemsets)\n",
    "\n",
    "transactions = []\n",
    "\n",
    "unique_users = np.unique(user_movie_data[:, 0])\n",
    "\n",
    "for user in unique_users:\n",
    "\n",
    "    user_movies = user_movie_data[user_movie_data[:, 0] == user][:, 1]\n",
    "\n",
    "    transactions.append(list(user_movies))\n",
    "\n",
    "# Using the apriori algorithm\n",
    "frequent_itemsets = Apriori(transactions, min=0.2)\n",
    "\n",
    "frequent_itemsets[:5]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
